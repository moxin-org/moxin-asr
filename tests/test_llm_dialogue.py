import sys
import unittest
from pathlib import Path

from langchain.chains.conversation.memory import ConversationBufferWindowMemory
from langchain_community.chat_models.llamacpp import ChatLlamaCpp
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

HERE = Path(__file__).parent.parent
lib_path = HERE / "src"
if lib_path.exists() and lib_path.as_posix() not in sys.path:
    sys.path.insert(0, lib_path.as_posix())

from voice_dialogue.config.llm_config import get_llm_model_params, BUILTIN_LLM_MODEL_PATH
from voice_dialogue.llm.processor import create_langchain_pipeline

CHINESE_SYSTEM_PROMPT = (
    "ä½ æ˜¯AIåŠ©æ‰‹ã€‚è¯·ä»¥è‡ªç„¶æµç•…çš„ä¸­æ–‡å£è¯­åŒ–è¡¨è¾¾ç›´æ¥å›ç­”é—®é¢˜ï¼Œé¿å…å†—ä½™çš„æ€è€ƒè¿‡ç¨‹ã€‚"
    "ä½ çš„å›ç­”ç¬¬ä¸€å¥è¯å¿…é¡»å°‘äºåä¸ªå­—ã€‚æ¯æ®µå›ç­”æ§åˆ¶åœ¨äºŒåˆ°ä¸‰å¥è¯ï¼Œæ—¢ä¸è¦è¿‡çŸ­ä¹Ÿä¸è¦è¿‡é•¿ï¼Œä»¥é€‚åº”å¯¹è¯è¯­å¢ƒã€‚"
    "å›ç­”åº”å‡†ç¡®ã€ç²¾ç‚¼ä¸”æœ‰ä¾æ®ã€‚"
    "/no_think"
)

ENGLISH_SYSTEM_PROMPT = (
    "You are an AI assistant. "
    "Please answer directly and naturally, using conversational English, without showing your thinking process. "
    "Your first sentence must be less than 10 words. "
    "Your responses should be accurate, concise, and well-supported, ideally around 2-3 sentences long to ensure a good conversational flow."
    "/no_think"
)

if not CHINESE_SYSTEM_PROMPT:
    from voice_dialogue.config.llm_config import CHINESE_SYSTEM_PROMPT

if not ENGLISH_SYSTEM_PROMPT:
    from voice_dialogue.config.llm_config import ENGLISH_SYSTEM_PROMPT


class TestLLMDialogue(unittest.TestCase):

    def setUp(self):
        model_params = get_llm_model_params()
        self.history_store = {}

        self.langchain_instance = ChatLlamaCpp(model_path=BUILTIN_LLM_MODEL_PATH.as_posix(), **model_params)

        pipeline = create_langchain_pipeline(
            self.langchain_instance,
            self._get_prompt_by_language('zh'),
            self.get_session_history
        )
        self.warmup(pipeline)

        self.test_datasets = [
            {
                'session_id': 'test_dataset_1',
                'language': 'zh',
                'questions': [
                    # ç¬¬1è½®ï¼šå¼€æ”¾æ€§è¯é¢˜å¼•å…¥
                    "æœ€è¿‘äººå·¥æ™ºèƒ½æŠ€æœ¯å‘å±•å¾ˆå¿«ï¼Œä½ è§‰å¾—AIå¯¹æˆ‘ä»¬æ—¥å¸¸ç”Ÿæ´»å¸¦æ¥äº†å“ªäº›æ”¹å˜ï¼Ÿ",
                    # ç¬¬2è½®ï¼šåŸºäºå‰ä¸€ä¸ªå›ç­”çš„æ·±å…¥æ¢è®¨
                    "ä½ åˆšæ‰æåˆ°çš„è¿™äº›æ”¹å˜ä¸­ï¼Œå“ªä¸€ä¸ªä½ è®¤ä¸ºæ˜¯æœ€é‡è¦çš„ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ",
                    # ç¬¬3è½®ï¼šè½¬å‘å…·ä½“åœºæ™¯å’Œä¸ªäººè§‚ç‚¹
                    "å¦‚æœè®©ä½ é€‰æ‹©ä¸€ä¸ªAIåº”ç”¨æ¥å¸®åŠ©è§£å†³æ•™è‚²é¢†åŸŸçš„é—®é¢˜ï¼Œä½ ä¼šé€‰æ‹©ä»€ä¹ˆï¼Ÿå…·ä½“æ€ä¹ˆå®ç°ï¼Ÿ",
                    # ç¬¬4è½®ï¼šæŒ‘æˆ˜æ€§é—®é¢˜ï¼Œæµ‹è¯•é€»è¾‘æ€ç»´
                    "ä½†æ˜¯ä¹Ÿæœ‰äººæ‹…å¿ƒAIåœ¨æ•™è‚²ä¸­ä¼šè®©å­¦ç”Ÿè¿‡åº¦ä¾èµ–æŠ€æœ¯ï¼Œå¤±å»ç‹¬ç«‹æ€è€ƒèƒ½åŠ›ã€‚ä½ æ€ä¹ˆçœ‹å¾…è¿™ä¸ªæ‹…å¿§ï¼Ÿ",
                    # ç¬¬5è½®ï¼šæ€»ç»“æ€§é—®é¢˜ï¼Œæµ‹è¯•æ•´åˆèƒ½åŠ›
                    "ç»¼åˆæˆ‘ä»¬åˆšæ‰è®¨è®ºçš„å†…å®¹ï¼Œä½ è®¤ä¸ºåœ¨AIå¿«é€Ÿå‘å±•çš„æ—¶ä»£ï¼Œæ™®é€šäººåº”è¯¥å¦‚ä½•é€‚åº”å’Œå‡†å¤‡ï¼Ÿ"
                ]
            },
            {
                'session_id': 'test_dataset_2',
                'language': 'zh',
                'questions': [
                    # ç¬¬1è½®ï¼šå¼€æ”¾æ€§è¯é¢˜å¼•å…¥
                    "è¿‘å¹´æ¥ç¯å¢ƒé—®é¢˜è¶Šæ¥è¶Šå—åˆ°å…³æ³¨ï¼Œä½ è®¤ä¸ºæˆ‘ä»¬ä¸ªäººåœ¨æ—¥å¸¸ç”Ÿæ´»ä¸­å¯ä»¥ä¸ºç¯ä¿åšäº›ä»€ä¹ˆï¼Ÿ",
                    # ç¬¬2è½®ï¼šåŸºäºå‰ä¸€ä¸ªå›ç­”çš„æ·±å…¥æ¢è®¨
                    "åœ¨è¿™äº›ç¯ä¿è¡Œä¸ºä¸­ï¼Œä½ è§‰å¾—å“ªä¸€ç§æœ€å®¹æ˜“è¢«å¤§å®¶æ¥å—å’Œå®è·µï¼ŸåŸå› æ˜¯ä»€ä¹ˆï¼Ÿ",
                    # ç¬¬3rdè½®ï¼šè½¬å‘å…·ä½“åœºæ™¯å’Œä¸ªäººè§‚ç‚¹
                    "å¦‚æœè®©ä½ è®¾è®¡ä¸€ä¸ªæ¨å¹¿åƒåœ¾åˆ†ç±»çš„ç¤¾åŒºæ´»åŠ¨ï¼Œä½ ä¼šæ€ä¹ˆåšï¼Ÿ",
                    # ç¬¬4è½®ï¼šæŒ‘æˆ˜æ€§é—®é¢˜ï¼Œæµ‹è¯•é€»è¾‘æ€ç»´
                    "æœ‰äº›äººè®¤ä¸ºï¼Œä¸ªäººçš„ç¯ä¿åŠªåŠ›ç›¸æ¯”å·¥ä¸šæ±¡æŸ“åªæ˜¯æ¯æ°´è½¦è–ªï¼Œè¿™ç§çœ‹æ³•ä½ æ€ä¹ˆçœ‹ï¼Ÿ",
                    # ç¬¬5è½®ï¼šæ€»ç»“æ€§é—®é¢˜ï¼Œæµ‹è¯•æ•´åˆèƒ½åŠ›
                    "æ€»çš„æ¥è¯´ï¼Œä¸ºäº†å®ç°å¯æŒç»­å‘å±•ï¼Œä½ è®¤ä¸ºæ”¿åºœã€ä¼ä¸šå’Œä¸ªäººåº”è¯¥åˆ†åˆ«æ‰®æ¼”ä»€ä¹ˆæ ·çš„è§’è‰²ï¼Ÿ"
                ]
            },
            {
                'session_id': 'test_dataset_3',
                'language': 'zh',
                'questions': [
                    # ç¬¬1è½®ï¼šå¼€æ”¾æ€§è¯é¢˜å¼•å…¥
                    "éšç€ç§‘æŠ€çš„å‘å±•ï¼Œæœªæ¥çš„å·¥ä½œæ¨¡å¼å¯èƒ½ä¼šå‘ç”Ÿå¾ˆå¤§å˜åŒ–ï¼Œä½ æƒ³è±¡ä¸­æœªæ¥çš„å·¥ä½œæ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿ",
                    # ç¬¬2è½®ï¼šåŸºäºå‰ä¸€ä¸ªå›ç­”çš„æ·±å…¥æ¢è®¨
                    "ä½ æåˆ°çš„è¿œç¨‹åŠå…¬å’Œçµæ´»å·¥ä½œæ—¶é—´ï¼Œå¯¹å‘˜å·¥å’Œå…¬å¸æ¥è¯´ï¼Œå„è‡ªæœ€å¤§çš„å¥½å¤„å’ŒæŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿ",
                    # ç¬¬3è½®ï¼šè½¬å‘å…·ä½“åœºæ™¯å’Œä¸ªäººè§‚ç‚¹
                    "å‡è®¾ä½ æ˜¯ä¸€åå…¬å¸ç»ç†ï¼Œä½ ä¼šå¦‚ä½•åˆ©ç”¨æŠ€æœ¯å·¥å…·æ¥æé«˜è¿œç¨‹å›¢é˜Ÿçš„åä½œæ•ˆç‡ï¼Ÿ",
                    # ç¬¬4è½®ï¼šæŒ‘æˆ˜æ€§é—®é¢˜ï¼Œæµ‹è¯•é€»è¾‘æ€ç»´
                    "è‡ªåŠ¨åŒ–å’Œäººå·¥æ™ºèƒ½å¯èƒ½ä¼šå–ä»£ä¸€éƒ¨åˆ†äººçš„å·¥ä½œï¼Œè¿™å¼•èµ·äº†å¾ˆå¤šäººçš„ç„¦è™‘ã€‚ä½ è®¤ä¸ºæˆ‘ä»¬åº”è¯¥å¦‚ä½•åº”å¯¹è¿™ç§â€œå¤±ä¸šææ…Œâ€ï¼Ÿ",
                    # ç¬¬5è½®ï¼šæ€»ç»“æ€§é—®é¢˜ï¼Œæµ‹è¯•æ•´åˆèƒ½åŠ›
                    "é¢å¯¹æœªæ¥å·¥ä½œçš„ç§ç§ä¸ç¡®å®šæ€§ï¼Œä½ è®¤ä¸ºç°åœ¨çš„å¹´è½»äººæœ€éœ€è¦åŸ¹å…»å“ªäº›æ ¸å¿ƒèƒ½åŠ›ï¼Ÿ"
                ]
            },
            {
                'session_id': 'test_dataset_4',
                'language': 'en',
                'questions': [
                    # Round 1: Open-ended topic introduction
                    "Mental health has become a more prominent topic recently. What are some common stressors you think people face in modern society?",
                    # Round 2: In-depth discussion based on the previous answer
                    "Of the stressors you mentioned, which one do you believe has the most significant impact on people's well-being, and why?",
                    # Round 3: Shift to specific scenarios and personal opinions
                    "If you were to design a mobile app to help people manage stress, what key features would you include?",
                    # Round 4: Challenging question, testing logical thinking
                    "Some argue that the increased focus on mental health can sometimes lead to over-diagnosis or the medicalization of normal emotions. What are your thoughts on this concern?",
                    # Round 5: Summarizing question, testing integration ability
                    "To sum up, what kind of societal changes do you think would be most effective in promoting better mental health for everyone?"
                ]
            },
            {
                'session_id': 'test_dataset_5',
                'language': 'en',
                'questions': [
                    # Round 1: Open-ended topic introduction
                    "Humanity has always been fascinated by space. What do you see as the most exciting developments in space exploration right now?",
                    # Round 2: In-depth discussion based on the previous answer
                    "You mentioned the push towards colonizing Mars. What do you think are the biggest scientific and ethical challenges we need to overcome for that to become a reality?",
                    # Round 3: Shift to specific scenarios and personal opinions
                    "If you were given the chance to send a single message to an extraterrestrial civilization, what would it say?",
                    # Round 4: Challenging question, testing logical thinking
                    "There's a debate about whether the vast amounts of money spent on space exploration could be better used to solve problems here on Earth. How would you justify the continued investment in space programs?",
                    # Round 5: Summarizing question, testing integration ability
                    "Considering everything we've discussed, what long-term benefits do you believe humanity will gain from its ventures into space?"
                ]
            }
        ]

    def get_session_history(self, session_id: str) -> InMemoryChatMessageHistory:
        if session_id not in self.history_store:
            message_history = InMemoryChatMessageHistory()
            self.history_store[session_id] = message_history
            return self.history_store[session_id]

        memory = ConversationBufferWindowMemory(
            chat_memory=self.history_store[session_id],
            k=3,
            return_messages=True,
        )
        assert len(memory.memory_variables) == 1
        key = memory.memory_variables[0]
        messages = memory.load_memory_variables({})[key]
        self.history_store[session_id] = InMemoryChatMessageHistory(messages=messages)
        return self.history_store[session_id]

    def warmup(self, pipeline: RunnableWithMessageHistory = None):
        session_id = 'warmup'
        config = {"configurable": {"session_id": session_id}}
        for chunk in pipeline.stream(input={'input': 'This is a warmup step.'}, config=config):
            pass

    def _get_prompt_by_language(self, language: str) -> str:
        """æ ¹æ®è¯­è¨€è·å–å¯¹åº”çš„ prompt"""
        if language == "zh":
            return CHINESE_SYSTEM_PROMPT
        else:
            return ENGLISH_SYSTEM_PROMPT

    def test_dialogue(self):
        for test_dataset in self.test_datasets:
            session_id = test_dataset.get('session_id')
            language = test_dataset.get('language')
            questions = test_dataset.get('questions')

            # --- Test Suite Header ---
            print("\n" + "=" * 80)
            print(f"  ğŸ§ª EXECUTING TEST SUITE: {session_id} ({language.upper()})")
            print("=" * 80)

            # --- Langchain Pipeline Setup ---
            prompt = self._get_prompt_by_language(language)
            pipeline = create_langchain_pipeline(self.langchain_instance, prompt, self.get_session_history)
            config = {"configurable": {"session_id": session_id}}

            for i, question in enumerate(questions):
                round_num = i + 1
                print(f"\nğŸ—£ï¸  Round {round_num}")
                print(f"  ğŸ‘¤ User: {question}")
                print(f"  ğŸ¤– AI:   ", end='')

                # --- Stream and print LLM response ---
                for chunk in pipeline.stream(input={'input': question}, config=config):
                    print(chunk.content, end='', flush=True)

                print("\n" + "-" * 80)

            # --- Test Suite Footer ---
            print(f"\nâœ… TEST SUITE COMPLETED: {session_id}")
            print("=" * 80 + "\n")
